# DTRNet Experiment Configuration for FineWeb 15B PT

### Model Arguments
model_name_or_path:  #provide config path here 
tokenizer_name_or_path: # provide tokenizer path
custom_model: false
trust_remote_code: true
attn_implementation: flash_attention_2
torch_dtype: bfloat16

is_dtrnet: true # If DTRNet model or not
is_mod: false
is_dllm: false
dtrnet_layers: [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24 , 26, 28, 30] # layers to be changed to DTRNet [DTRNet]
aux_loss_coeff: 0.0008 # Coefficient for auxiliary loss (lambda) [DTRNet, D-LLM]
topk: 0.7 # Top-k value for MoD [MoD]
dynamic_reserve_initials: 2 # Initial dynamic reserve for D-LLM [D-LLM]
dynamic_active_target: 0.7 # Acceleration rate of tokens in D-LLM [D-LLM]


save_modeling_code: None #src/transformers_extra/models/custom_llama
auto_map:
  "AutoModelForCausalLM": "modeling_custom_llama.CustomLlamaForCausalLM"
  "AutoConfig": "configuration_custom_llama.CustomLlamaConfig"

# Data Arguments - using a small dataset for testing
dataset_name: HuggingFaceFW/fineweb-edu # Example dataset
dataset_config: sample-100BT
dataset_train_split: train
dataset_test_split: test
max_seq_length: 2048
packing: true
dataset_num_proc: 128
dataloader_num_workers: 128
processed_dataset_dir: /work/aman/data/fineweb_15B_tokenized_fla  # Provide processed data path here else None

### Training parameters
num_train_epochs: 1
per_device_train_batch_size: 6
per_device_eval_batch_size: 8
gradient_accumulation_steps: 16
learning_rate: 3.0e-4
lr_scheduler_type: cosine_with_min_lr
lr_scheduler_kwargs:
  min_lr: 1.0e-5
warmup_ratio: 0.1
weight_decay: 0.01
torch_compile: false

# Optimization
optim: adamw_torch
bf16: true
fp16: false
use_liger_kernel: false

# Evaluation and saving
eval_strategy: "no"
eval_steps: 500
save_strategy: steps
save_steps: 10
save_total_limit: 5

# Logging
logging_strategy: steps
logging_steps: 10
report_to: [wandb]

# Others
remove_unused_columns: true
dataloader_num_workers: 4
seed: 42
push_to_hub: false

# Gradient checkpointing for memory efficiency
gradient_checkpointing: false


